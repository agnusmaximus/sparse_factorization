{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/scratch0/maxlam/env3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "%matplotlib inline\n",
    "from SparseFactorization.sparse_factorization import *\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define roots of unity\n",
    "def w_n(n):\n",
    "    return np.e**((2 * np.pi * 1j) / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. +0.j ,  0. +0.j ,  0. +0.j ],\n",
       "       [ 0. +0.j , -0.5-0.9j,  0. +0.j ],\n",
       "       [ 0. +0.j ,  0. +0.j , -0.5+0.9j]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Omegas\n",
    "def Omega_n(n):\n",
    "    values = [w_n(n)**(-i) if i >= 1 else 1 for i in range(n)]\n",
    "    return np.diag(values)\n",
    "\n",
    "Omega_n(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.+0.0000000e+00j,  0.+0.0000000e+00j,  1.+0.0000000e+00j,\n",
       "         0.+0.0000000e+00j],\n",
       "       [ 0.+0.0000000e+00j,  1.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "        -1.-1.2246468e-16j],\n",
       "       [ 1.+0.0000000e+00j,  0.+0.0000000e+00j, -1.-0.0000000e+00j,\n",
       "        -0.-0.0000000e+00j],\n",
       "       [ 0.+0.0000000e+00j,  1.+0.0000000e+00j, -0.-0.0000000e+00j,\n",
       "         1.+1.2246468e-16j]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define B_2 and B_4\n",
    "def B_n(n):\n",
    "    \n",
    "    # Identities\n",
    "    ident = np.eye(n//2)\n",
    "    stacked_idnt = np.concatenate([ident, ident], axis=0)\n",
    "    \n",
    "    # Omegas\n",
    "    o1 = Omega_n(n//2)\n",
    "    o2 = -Omega_n(n//2)\n",
    "    stacked_omeg = np.concatenate([o1, o2], axis=0)\n",
    "    \n",
    "    return np.concatenate([stacked_idnt, stacked_omeg], axis=1)\n",
    "\n",
    "B_n(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j,  6.1232340e-17-1.0000000e+00j,\n",
       "         -1.0000000e+00-1.2246468e-16j, -1.8369702e-16+1.0000000e+00j,\n",
       "         -1.0000000e+00+0.0000000e+00j, -6.1232340e-17+1.0000000e+00j,\n",
       "          1.0000000e+00+1.2246468e-16j,  1.8369702e-16-1.0000000e+00j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j, -1.0000000e+00-1.2246468e-16j,\n",
       "         -1.0000000e+00+0.0000000e+00j,  1.0000000e+00+1.2246468e-16j,\n",
       "          1.0000000e+00+0.0000000e+00j, -1.0000000e+00-1.2246468e-16j,\n",
       "         -1.0000000e+00+0.0000000e+00j,  1.0000000e+00+1.2246468e-16j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j, -1.8369702e-16+1.0000000e+00j,\n",
       "          1.0000000e+00+1.2246468e-16j, -3.0616170e-16+1.0000000e+00j,\n",
       "         -1.0000000e+00+0.0000000e+00j,  1.8369702e-16-1.0000000e+00j,\n",
       "         -1.0000000e+00-1.2246468e-16j,  3.0616170e-16-1.0000000e+00j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j, -1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j, -1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j, -1.0000000e+00+0.0000000e+00j,\n",
       "          1.0000000e+00+0.0000000e+00j, -1.0000000e+00+0.0000000e+00j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j, -6.1232340e-17+1.0000000e+00j,\n",
       "         -1.0000000e+00-1.2246468e-16j,  1.8369702e-16-1.0000000e+00j,\n",
       "         -1.0000000e+00+0.0000000e+00j,  6.1232340e-17-1.0000000e+00j,\n",
       "          1.0000000e+00+1.2246468e-16j, -1.8369702e-16+1.0000000e+00j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j,  1.0000000e+00+1.2246468e-16j,\n",
       "         -1.0000000e+00+0.0000000e+00j, -1.0000000e+00-1.2246468e-16j,\n",
       "          1.0000000e+00+0.0000000e+00j,  1.0000000e+00+1.2246468e-16j,\n",
       "         -1.0000000e+00+0.0000000e+00j, -1.0000000e+00-1.2246468e-16j],\n",
       "        [ 1.0000000e+00+0.0000000e+00j,  1.8369702e-16-1.0000000e+00j,\n",
       "          1.0000000e+00+1.2246468e-16j,  3.0616170e-16-1.0000000e+00j,\n",
       "         -1.0000000e+00+0.0000000e+00j, -1.8369702e-16+1.0000000e+00j,\n",
       "         -1.0000000e+00-1.2246468e-16j, -3.0616170e-16+1.0000000e+00j]]),\n",
       " [array([[ 1.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           1.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  6.1232340e-17-1.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           1.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "          -1.0000000e+00-1.2246468e-16j,  0.0000000e+00+0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j, -1.8369702e-16+1.0000000e+00j],\n",
       "         [ 1.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "          -1.0000000e+00-0.0000000e+00j, -0.0000000e+00-0.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j, -0.0000000e+00-0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j, -6.1232340e-17+1.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j, -0.0000000e+00-0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           1.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j, -0.0000000e+00-0.0000000e+00j,\n",
       "           1.0000000e+00+1.2246468e-16j, -0.0000000e+00-0.0000000e+00j],\n",
       "         [ 0.0000000e+00+0.0000000e+00j,  0.0000000e+00+0.0000000e+00j,\n",
       "           0.0000000e+00+0.0000000e+00j,  1.0000000e+00+0.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j, -0.0000000e+00-0.0000000e+00j,\n",
       "          -0.0000000e+00-0.0000000e+00j,  1.8369702e-16-1.0000000e+00j]]),\n",
       "  array([[ 1.+0.0000000e+00j,  0.+0.0000000e+00j,  1.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "         [ 0.+0.0000000e+00j,  1.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "          -1.-1.2246468e-16j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "         [ 1.+0.0000000e+00j,  0.+0.0000000e+00j, -1.-0.0000000e+00j,\n",
       "          -0.-0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "         [ 0.+0.0000000e+00j,  1.+0.0000000e+00j, -0.-0.0000000e+00j,\n",
       "           1.+1.2246468e-16j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "         [ 0.+0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  1.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           1.+0.0000000e+00j,  0.+0.0000000e+00j],\n",
       "         [ 0.+0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j,  1.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j, -1.-1.2246468e-16j],\n",
       "         [ 0.+0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  1.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "          -1.-0.0000000e+00j, -0.-0.0000000e+00j],\n",
       "         [ 0.+0.0000000e+00j,  0.+0.0000000e+00j,  0.+0.0000000e+00j,\n",
       "           0.+0.0000000e+00j,  0.+0.0000000e+00j,  1.+0.0000000e+00j,\n",
       "          -0.-0.0000000e+00j,  1.+1.2246468e-16j]]),\n",
       "  array([[ 1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 1., -1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., -1.,  0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  1., -1.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "         [ 0.,  0.,  0.,  0.,  0.,  0.,  1., -1.]]),\n",
       "  array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1]])])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define F_8 and F_4\n",
    "def F_8():\n",
    "    # First b8 factor\n",
    "    B_8 = B_n(8)\n",
    "    \n",
    "    # Stack b4s\n",
    "    B_4 = B_n(4)\n",
    "    rows = []    \n",
    "    for i in range(2):\n",
    "        row = [np.zeros(B_4.shape) for j in range(i)] + [B_4] + [np.zeros(B_4.shape) for j in range(2-i-1)]\n",
    "        rows.append(np.concatenate(row, axis=1))\n",
    "    B_4_factor = np.concatenate(rows, axis=0)\n",
    "    \n",
    "    # Stack b2s\n",
    "    B_2 = B_n(2)\n",
    "    rows = []\n",
    "    for i in range(4):\n",
    "        row = [np.zeros(B_2.shape) for j in range(i)] + [B_2] + [np.zeros(B_2.shape) for j in range(4-i-1)]\n",
    "        rows.append(np.concatenate(row, axis=1))\n",
    "    B_2_factor = np.concatenate(rows, axis=0)\n",
    "    \n",
    "    # Permutation matrix\n",
    "    perm = np.array([\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 1, 0 ,0 ,0 ,0 ,0],\n",
    "        [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    return B_8.dot(B_4_factor).dot(B_2_factor).dot(perm), [B_8, B_4_factor, B_2_factor, perm]\n",
    "    \n",
    "F_8()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to convert complex matrices into pytorch form (n,m,...,2)\n",
    "def to_pytorch(complex_matrix):\n",
    "    return np.stack([complex_matrix.real, complex_matrix.imag], axis=len(complex_matrix.shape))\n",
    "\n",
    "# Helper to convert complex matrices in pytorch form into numpy\n",
    "def to_numpy(complex_matrix):\n",
    "    newshp = complex_matrix.shape[:-1]\n",
    "    return complex_matrix[:,:,0].reshape(newshp) + complex_matrix[:,:,-1].reshape(newshp)*1j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[[[1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]]\n",
      "\n",
      " [[1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]]]\n",
      "[[[1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [1 1]]]\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "[[[1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]]]\n",
      "First factorization pass\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 758.1, Loss: 758.1, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0897598, Loss: 0.0897598, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.127691, Loss: 0.127691, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0998913, Loss: 0.0998913, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0862622, Loss: 0.0862622, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.104723, Loss: 0.104723, Variables NNzs: [32, 32, 32, 8], Sum NNzs: 104\n",
      "==================\n",
      "Compare real parts\n",
      "==================\n",
      "Actual 0\n",
      " [[ 1.0e+00  0.0e+00  0.0e+00  0.0e+00  1.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  1.0e+00  0.0e+00  0.0e+00  0.0e+00  6.1e-17  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  1.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  1.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.8e-16]\n",
      " [ 1.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.0e+00 -0.0e+00 -0.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  1.0e+00  0.0e+00  0.0e+00 -0.0e+00 -6.1e-17 -0.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  1.0e+00  0.0e+00 -0.0e+00 -0.0e+00  1.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  1.0e+00 -0.0e+00 -0.0e+00 -0.0e+00  1.8e-16]]\n",
      "Recovered 0\n",
      " [[ 1.4  0.   0.   0.  -0.8  0.   0.   0. ]\n",
      " [ 0.   0.2  0.   0.   0.  -0.7 -0.   0. ]\n",
      " [ 0.  -0.  -1.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.  -0.5  0.   0.   0.   0.7]\n",
      " [ 1.4  0.   0.   0.   0.8  0.  -0.   0. ]\n",
      " [ 0.   0.2  0.   0.  -0.   0.7  0.  -0. ]\n",
      " [ 0.  -0.  -1.   0.   0.  -0.  -0.  -0. ]\n",
      " [ 0.   0.   0.  -0.5  0.  -0.  -0.  -0.7]]\n",
      "Actual 1\n",
      " [[ 1.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  0.]\n",
      " [ 1.  0. -1. -0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0. -1.]\n",
      " [ 0.  0.  0.  0.  1.  0. -1. -0.]\n",
      " [ 0.  0.  0.  0.  0.  1. -0.  1.]]\n",
      "Recovered 1\n",
      " [[ 0.4 -0.   0.1  0.  -0.  -0.   0.   0. ]\n",
      " [-0.  -1.   0.   0.3 -0.   0.   0.   0. ]\n",
      " [-0.7  0.   0.3  0.   0.   0.  -0.   0. ]\n",
      " [ 0.  -0.9  0.   0.9  0.   0.   0.   0. ]\n",
      " [-0.   0.  -0.   0.   0.5  0.   0.1 -0. ]\n",
      " [-0.   0.   0.   0.   0.  -0.8 -0.   0.9]\n",
      " [ 0.   0.  -0.   0.  -0.4  0.   1.8  0. ]\n",
      " [ 0.   0.  -0.   0.   0.  -0.8  0.  -0.8]]\n",
      "Actual 2\n",
      " [[ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "Recovered 2\n",
      " [[ 0.7  0.7  0.   0.  -0.   0.   0.   0. ]\n",
      " [ 0.2 -0.2  0.   0.  -0.   0.  -0.   0. ]\n",
      " [ 0.   0.   0.9  0.9  0.   0.   0.   0. ]\n",
      " [-0.   0.  -1.7  1.7 -0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.  -1.6 -1.6  0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.9 -0.9 -0.   0. ]\n",
      " [ 0.   0.   0.   0.  -0.  -0.   0.4  0.4]\n",
      " [ 0.   0.   0.   0.   0.  -0.   0.1 -0.1]]\n",
      "Actual 3\n",
      " [[1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Recovered 3\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "==================\n",
      "Compare imag parts\n",
      "==================\n",
      "Actual 0\n",
      " [[ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.2e-16  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  1.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00 -0.0e+00 -0.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00  1.0e+00 -0.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00 -0.0e+00  1.2e-16 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00 -0.0e+00 -0.0e+00 -1.0e+00]]\n",
      "Recovered 0\n",
      " [[ 0.4  0.   0.   0.   0.6  0.   0.   0. ]\n",
      " [ 0.   0.9  0.   0.   0.   0.6  0.   0. ]\n",
      " [ 0.   0.  -0.1  0.   0.   0.   0.6  0. ]\n",
      " [ 0.   0.   0.   0.2  0.   0.   0.  -0.7]\n",
      " [ 0.4  0.   0.   0.  -0.6  0.   0.   0. ]\n",
      " [ 0.   0.9  0.   0.   0.  -0.6  0.   0. ]\n",
      " [ 0.   0.  -0.1  0.   0.   0.  -0.6  0. ]\n",
      " [ 0.   0.   0.   0.2  0.   0.   0.   0.7]]\n",
      "Actual 1\n",
      " [[ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00 -1.2e-16  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00 -0.0e+00 -0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00 -0.0e+00  1.2e-16  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00 -1.2e-16]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00 -0.0e+00]\n",
      " [ 0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00  0.0e+00 -0.0e+00  1.2e-16]]\n",
      "Recovered 1\n",
      " [[-0.6  0.  -0.4  0.   0.   0.   0.   0. ]\n",
      " [ 0.  -0.5  0.  -0.5  0.   0.   0.   0. ]\n",
      " [ 0.7  0.  -0.6  0.   0.   0.   0.   0. ]\n",
      " [ 0.   1.7  0.   0.6  0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.1  0.  -1.1  0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.8  0.   0.6]\n",
      " [ 0.   0.   0.   0.  -1.   0.  -1.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.8  0.  -0.6]]\n",
      "Actual 2\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Recovered 2\n",
      " [[ 0.6  0.6  0.   0.   0.   0.   0.   0. ]\n",
      " [ 1.  -1.   0.   0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   1.3  1.3  0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.5 -0.5  0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.  -0.7 -0.7  0.   0. ]\n",
      " [ 0.   0.   0.   0.  -0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.  -0.7 -0.7]\n",
      " [ 0.   0.   0.   0.   0.   0.  -1.   1. ]]\n",
      "Actual 3\n",
      " [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Recovered 3\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "# This experiment factorized F8 using fine grained sparsity constraints.  \n",
    "# Given the true factors of F_8, we force all nnzs in the corresponding optimized       \n",
    "# matrix to be 0 if it is also a 0 in the true factor\n",
    "#\n",
    "# E.g: if\n",
    "# F_4_1 = [[1, 0, 1, 0],\n",
    "#          [0, 1, 0, w4],\n",
    "#          [1, 0, -1, 0],\n",
    "#          [0, 1, 0, -w4]\n",
    "# where F_4 = F_4_1 * F_4_2 * F_4_perm\n",
    "# Then we force factor 1 the optimize matrix to have form (enforcing sparsity constraint)\n",
    "# \n",
    "# [[a, 0, b, 0],\n",
    "#  [0, c, 0, d],\n",
    "#  [e, 0, f, 0],\n",
    "#  [0, g, 0, h]\n",
    "#\n",
    "# Since the 0s all match, I call this the fine-grained sparsity constraint\n",
    "    \n",
    "\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import SparseFactorization\n",
    "from SparseFactorization.sparse_factorization import *\n",
    "\n",
    "F_8_matrix, F_8_true_factors = F_8()\n",
    "\n",
    "# Sparsity enforcements\n",
    "B_8_sparsity = (F_8_true_factors[0] != 0) * 1\n",
    "B_8_sparsity = np.stack([B_8_sparsity, B_8_sparsity], axis=2)\n",
    "B_4_sparsity = (F_8_true_factors[1] != 0) * 1\n",
    "B_4_sparsity = np.stack([B_4_sparsity, B_4_sparsity], axis=2)\n",
    "B_2_sparsity = (F_8_true_factors[2] != 0) * 1\n",
    "B_2_sparsity = np.stack([B_2_sparsity, B_2_sparsity], axis=2)\n",
    "\n",
    "# Permutation enforce\n",
    "perm_values = F_8_true_factors[-1]\n",
    "perm_values = to_pytorch(perm_values)\n",
    "\n",
    "# Dbg\n",
    "print(B_8_sparsity)\n",
    "print(B_4_sparsity)\n",
    "print(B_2_sparsity)\n",
    "print(perm_values)\n",
    "\n",
    "# Factorize!\n",
    "hyperparameters = {\n",
    "    \"learning_rate\" : 1e-2,\n",
    "    \"l1_parameter\" : 0, \n",
    "    \"pruning_threshold\" : 0, \n",
    "    \"training_iters\" : 5000,\n",
    "    \"log_every\" : 1000,\n",
    "    \n",
    "    # Here we 4 factors (XYZP)\n",
    "    \"matrix_initializations\" : {\n",
    "        #0: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "        #1: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "        #2: np.stack([np.eye(8), np.eye(8)], axis=2),        \n",
    "        #3: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "        0: np.stack([np.random.normal(0, 1, (8,8)), np.random.normal(0, 1, (8,8))], axis=2),\n",
    "        1: np.stack([np.random.normal(0, 1, (8,8)), np.random.normal(0, 1, (8,8))], axis=2),\n",
    "        2: np.stack([np.random.normal(0, 1, (8,8)), np.random.normal(0, 1, (8,8))], axis=2),        \n",
    "        3: np.stack([np.random.normal(0, 1, (8,8)), np.random.normal(0, 1, (8,8))], axis=2),        \n",
    "    },\n",
    "    \n",
    "    # Here we apply sparsity constraints according to the sparsity of the butterfly patterns\n",
    "    \"sparsity_constraints\" : {\n",
    "        0:B_8_sparsity,\n",
    "        1:B_4_sparsity,\n",
    "        2:B_2_sparsity,\n",
    "    },\n",
    "    \n",
    "    # Here we apply value constraints to the permutation matrix. P=permutation matrix\n",
    "    \"value_constraints\" : {\n",
    "        3: perm_values\n",
    "    },\n",
    "    \n",
    "    \"is_complex\": True\n",
    "}\n",
    "\n",
    "# First pass\n",
    "print(\"First factorization pass\")\n",
    "factorizer = SparseFactorizationWithEnforcedStructurePytorch(hyperparameters=hyperparameters)\n",
    "recovered_factors, details = factorizer.factorize(to_pytorch(F_8_matrix))\n",
    "\n",
    "# Second pass\n",
    "#print(\"Second factorization pass with lower learning rate\")\n",
    "#hyperparameters[\"matrix_initializations\"] = {i:v for i,v in enumerate(recovered_factors)}\n",
    "#hyperparameters[\"sparsity_constraints\"] = {\n",
    "#    0:B_8_sparsity,\n",
    "#    1:B_4_sparsity,\n",
    "#    2:B_2_sparsity,\n",
    "#}\n",
    "#factorizer = SparseFactorizationWithEnforcedStructurePytorch(hyperparameters=hyperparameters)\n",
    "#recovered_factors, details = factorizer.factorize(to_pytorch(F_8_matrix))\n",
    "\n",
    "recovered_factors = [to_numpy(x) for x in recovered_factors]\n",
    "\n",
    "# Visualize each of the recovered_factors and the actual factors\n",
    "actual_factors = F_8_true_factors\n",
    "\n",
    "# Put actual and recovered factors side by side\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare real parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].real)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].real)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].real)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].real)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].real)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].real)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].real)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].real)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare imag parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].imag)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].imag)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].imag)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].imag)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].imag)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].imag)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].imag)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "[[[1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 1]\n",
      "  [1 1]]]\n",
      "[[[1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [0 0]\n",
      "  [1 0]]]\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 13.8564, Loss: 13.9204, Variables NNzs: [128, 64, 32, 8], Sum NNzs: 232\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0485948, Loss: 0.134904, Variables NNzs: [122, 63, 28, 8], Sum NNzs: 221\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0456102, Loss: 0.131699, Variables NNzs: [122, 63, 28, 8], Sum NNzs: 221\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0502759, Loss: 0.136155, Variables NNzs: [118, 63, 28, 8], Sum NNzs: 217\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0497681, Loss: 0.135436, Variables NNzs: [118, 62, 28, 8], Sum NNzs: 216\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0495261, Loss: 0.135022, Variables NNzs: [116, 62, 28, 8], Sum NNzs: 214\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0501781, Loss: 0.135499, Variables NNzs: [116, 62, 28, 8], Sum NNzs: 214\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0514862, Loss: 0.136654, Variables NNzs: [116, 62, 28, 8], Sum NNzs: 214\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.0452708, Loss: 0.130248, Variables NNzs: [116, 62, 28, 8], Sum NNzs: 214\n",
      "==================\n",
      "Compare real parts\n",
      "==================\n",
      "Actual 0\n",
      " [[ 1.000e+00  0.000e+00  0.000e+00  0.000e+00  1.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  1.000e+00  0.000e+00  0.000e+00  0.000e+00  6.123e-17\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  1.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  -1.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  1.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00 -1.837e-16]\n",
      " [ 1.000e+00  0.000e+00  0.000e+00  0.000e+00 -1.000e+00 -0.000e+00\n",
      "  -0.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  1.000e+00  0.000e+00  0.000e+00 -0.000e+00 -6.123e-17\n",
      "  -0.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  1.000e+00  0.000e+00 -0.000e+00 -0.000e+00\n",
      "   1.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  1.000e+00 -0.000e+00 -0.000e+00\n",
      "  -0.000e+00  1.837e-16]]\n",
      "Recovered 0\n",
      " [[ 0.641  0.07   0.     0.014 -0.224 -0.174 -0.203  0.   ]\n",
      " [ 0.095  0.325  0.121 -0.027 -0.364  0.354  0.421 -0.432]\n",
      " [ 0.177  0.     0.347  0.14   0.     0.349 -0.261 -0.051]\n",
      " [ 0.109  0.195 -0.025  0.237  0.346 -0.392  0.237 -0.649]\n",
      " [ 0.641  0.07   0.     0.014  0.224  0.174  0.203  0.   ]\n",
      " [ 0.095  0.325  0.121 -0.027  0.364 -0.354 -0.421  0.432]\n",
      " [ 0.177  0.     0.347  0.14   0.    -0.349  0.261  0.051]\n",
      " [ 0.109  0.195 -0.025  0.237 -0.346  0.392 -0.237  0.649]]\n",
      "Actual 1\n",
      " [[ 1.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0. -1.  0.  0.  0.  0.]\n",
      " [ 1.  0. -1. -0.  0.  0.  0.  0.]\n",
      " [ 0.  1. -0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0. -1.]\n",
      " [ 0.  0.  0.  0.  1.  0. -1. -0.]\n",
      " [ 0.  0.  0.  0.  0.  1. -0.  1.]]\n",
      "Recovered 1\n",
      " [[ 1.24   0.145  0.12   0.112  0.     0.     0.     0.   ]\n",
      " [ 0.157  0.775  0.206 -0.142  0.     0.     0.     0.   ]\n",
      " [ 0.315 -0.089  0.819  0.237  0.     0.     0.     0.   ]\n",
      " [ 0.159  0.255 -0.12   0.791  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.78   0.177  0.169 -0.047]\n",
      " [ 0.     0.     0.     0.     0.     0.51  -0.023  0.333]\n",
      " [ 0.     0.     0.     0.    -0.192 -0.027  0.816  0.   ]\n",
      " [ 0.     0.     0.     0.    -0.331  0.262 -0.282  0.979]]\n",
      "Actual 2\n",
      " [[ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "Recovered 2\n",
      " [[ 1.232  0.183  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.119  0.917  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.837  0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.    -0.192  0.894  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.937  0.019  0.     0.   ]\n",
      " [ 0.     0.     0.     0.    -0.003  0.669  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.871  0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.    -0.223  1.131]]\n",
      "Actual 3\n",
      " [[1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1]]\n",
      "Recovered 3\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "==================\n",
      "Compare imag parts\n",
      "==================\n",
      "Actual 0\n",
      " [[ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00 -1.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  -1.225e-16  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  1.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00 -0.000e+00 -0.000e+00\n",
      "  -0.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00 -0.000e+00  1.000e+00\n",
      "  -0.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00 -0.000e+00 -0.000e+00\n",
      "   1.225e-16 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00 -0.000e+00 -0.000e+00\n",
      "  -0.000e+00 -1.000e+00]]\n",
      "Recovered 0\n",
      " [[-0.268 -0.386 -0.437 -0.349 -0.675 -0.355 -0.391 -0.242]\n",
      " [-0.379  0.65   0.216 -0.435  0.193 -0.498 -0.045  0.   ]\n",
      " [-0.349 -0.36   0.66   0.221  0.341  0.282 -0.656 -0.355]\n",
      " [-0.343  0.256 -0.354  0.718 -0.04   0.158  0.    -0.377]\n",
      " [-0.268 -0.386 -0.437 -0.349  0.675  0.355  0.391  0.242]\n",
      " [-0.379  0.65   0.216 -0.435 -0.193  0.498  0.045  0.   ]\n",
      " [-0.349 -0.36   0.66   0.221 -0.341 -0.282  0.656  0.355]\n",
      " [-0.343  0.256 -0.354  0.718  0.04  -0.158  0.     0.377]]\n",
      "Actual 1\n",
      " [[ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00 -1.225e-16  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00 -0.000e+00 -0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00 -0.000e+00  1.225e-16  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00  0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "   0.000e+00 -1.225e-16]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  -0.000e+00 -0.000e+00]\n",
      " [ 0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00  0.000e+00\n",
      "  -0.000e+00  1.225e-16]]\n",
      "Recovered 1\n",
      " [[ 0.512 -0.175 -0.366 -0.225  0.     0.     0.     0.   ]\n",
      " [-0.225  1.075  0.147 -0.33   0.     0.     0.     0.   ]\n",
      " [-0.256 -0.265  1.029  0.164  0.     0.     0.     0.   ]\n",
      " [-0.226  0.225 -0.251  1.05   0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.078  0.067  0.235  0.356]\n",
      " [ 0.     0.     0.     0.    -0.162  1.286  0.073 -0.149]\n",
      " [ 0.     0.     0.     0.    -0.292 -0.002  1.107  0.208]\n",
      " [ 0.     0.     0.     0.     0.044 -0.274 -0.012  0.857]]\n",
      "Actual 2\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Recovered 2\n",
      " [[ 0.738 -0.158  0.     0.     0.     0.     0.     0.   ]\n",
      " [-0.185  1.126  0.     0.     0.     0.     0.     0.   ]\n",
      " [ 0.     0.     1.166  0.21   0.     0.     0.     0.   ]\n",
      " [ 0.     0.    -0.079  1.127  0.     0.     0.     0.   ]\n",
      " [ 0.     0.     0.     0.     1.138  0.029  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     1.295  0.     0.   ]\n",
      " [ 0.     0.     0.     0.     0.     0.     1.133  0.222]\n",
      " [ 0.     0.     0.     0.     0.     0.     0.     0.891]]\n",
      "Actual 3\n",
      " [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n",
      "Recovered 3\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "# This experiment factorizes F8 using coarse grained sparsity constraints.  \n",
    "#\n",
    "# Given F_8 =\n",
    "# B_8 [[B4 0],[0, B4]] [[B2, 0, 0, 0], [0, B2, 0, 0], [0, 0, B2, 0], [0, 0, 0, B2]] Perm\n",
    "#\n",
    "# In this experiment we enforce the second and third factor to have 0s as in the form form above.\n",
    "# Since the 0s of the Is in the Bs are not captured these are called coarse grained sparsity constraints.\n",
    "\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import SparseFactorization\n",
    "from SparseFactorization.sparse_factorization import *\n",
    "\n",
    "F_8_matrix, F_8_true_factors = F_8()\n",
    "\n",
    "# Sparsity enforcements\n",
    "#B_8_sparsity = (F_8_true_factors[0] != 0) * 1\n",
    "#B_4_sparsity = (F_8_true_factors[1] != 0) * 1\n",
    "#B_2_sparsity = (F_8_true_factors[2] != 0) * 1\n",
    "\n",
    "# B_8 has no sparsity constraints\n",
    "# B_4 looks like: [B_4 0], [0, B4]\n",
    "B_4_sparsity = np.array([\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "    [0, 0, 0, 0, 1, 1, 1, 1],    \n",
    "])\n",
    "B_4_sparsity = np.stack([B_4_sparsity, B_4_sparsity], axis=2)\n",
    "\n",
    "B_2_sparsity = np.array([\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1],\n",
    "    [0, 0, 0, 0, 0, 0, 1, 1],    \n",
    "])\n",
    "B_2_sparsity = np.stack([B_2_sparsity, B_2_sparsity], axis=2)\n",
    "\n",
    "# Permutation enforce\n",
    "perm_values = to_pytorch(F_8_true_factors[-1])\n",
    "\n",
    "# Dbg\n",
    "#print(B_8_sparsity)\n",
    "print(B_4_sparsity)\n",
    "print(B_2_sparsity)\n",
    "print(perm_values)\n",
    "\n",
    "# Factorize!\n",
    "hyperparameters = {\n",
    "    \"learning_rate\" : 8e-3,\n",
    "    \"l1_parameter\" : .001, \n",
    "    \"pruning_threshold\" : 1e-3, \n",
    "    \"training_iters\" : 8000,\n",
    "    \"log_every\" : 1000,\n",
    "    \n",
    "    # Here we 4 factors (XYZP)\n",
    "    \"matrix_initializations\" : {\n",
    "        0: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "        1: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "        2: np.stack([np.eye(8), np.eye(8)], axis=2),        \n",
    "        3: np.stack([np.eye(8), np.eye(8)], axis=2),\n",
    "    },\n",
    "    \n",
    "    # Here we apply sparsity constraints according to the sparsity of the butterfly patterns\n",
    "    \"sparsity_constraints\" : {\n",
    "        #0:B_8_sparsity,\n",
    "        1:B_4_sparsity,\n",
    "        2:B_2_sparsity,\n",
    "    },\n",
    "    \n",
    "    # Here we apply value constraints to the permutation matrix. P=permutation matrix\n",
    "    \"value_constraints\" : {\n",
    "        3: perm_values\n",
    "    },\n",
    "    \n",
    "    \"is_complex\": True\n",
    "}\n",
    "\n",
    "factorizer = SparseFactorizationWithEnforcedStructurePytorch(hyperparameters=hyperparameters)\n",
    "recovered_factors, details = factorizer.factorize(to_pytorch(F_8_matrix))\n",
    "recovered_factors = [to_numpy(x) for x in recovered_factors]\n",
    "\n",
    "# Visualize each of the recovered_factors and the actual factors\n",
    "plt.figure(1, figsize=(15,15))\n",
    "\n",
    "actual_factors = F_8_true_factors\n",
    "\n",
    "# Put actual and recovered factors side by side\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare real parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].real)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].real)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].real)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].real)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].real)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].real)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].real)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].real)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare imag parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].imag)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].imag)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].imag)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].imag)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].imag)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].imag)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].imag)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. -1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1. -1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1. -1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1. -1.]]\n",
      "[array([[ 1.+0.000e+00j,  0.+0.000e+00j,  1.+0.000e+00j,  0.+0.000e+00j],\n",
      "       [ 0.+0.000e+00j,  1.+0.000e+00j,  0.+0.000e+00j, -1.-1.225e-16j],\n",
      "       [ 1.+0.000e+00j,  0.+0.000e+00j, -1.-0.000e+00j, -0.-0.000e+00j],\n",
      "       [ 0.+0.000e+00j,  1.+0.000e+00j, -0.-0.000e+00j,  1.+1.225e-16j]]), array([[ 1.,  1.,  0.,  0.],\n",
      "       [ 1., -1.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  1.],\n",
      "       [ 0.,  0.,  1., -1.]])]\n"
     ]
    }
   ],
   "source": [
    "# General F_n without the permutation matrix\n",
    "def block_n_k(small_n, mat_size):\n",
    "    if small_n == mat_size:\n",
    "        return B_n(small_n)\n",
    "    \n",
    "    # Stack bs\n",
    "    B = B_n(small_n)\n",
    "    rows = []    \n",
    "    for i in range(mat_size // small_n):\n",
    "        row = [np.zeros(B.shape) for j in range(i)] + [B] + [np.zeros(B.shape) for j in range((mat_size//small_n)-i-1)]\n",
    "        rows.append(np.concatenate(row, axis=1))\n",
    "    factor = np.concatenate(rows, axis=0)\n",
    "    return factor\n",
    "\n",
    "print(block_n_k(2, 8))\n",
    "    \n",
    "def F_n_no_perm(n):\n",
    "    assert n >= 4\n",
    "    factors = []\n",
    "    cur_n = n\n",
    "    while cur_n != 1:\n",
    "        factors.append(block_n_k(cur_n, n))\n",
    "        cur_n //= 2\n",
    "    \n",
    "    result = factors[0]\n",
    "    for f in factors[1:]:\n",
    "        result = result.dot(f)\n",
    "    return result, factors\n",
    "\n",
    "print(F_n_no_perm(4)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[[ 1.+0.0e+00j  1.+0.0e+00j  1.+0.0e+00j ...  1.+0.0e+00j  1.+0.0e+00j\n",
      "   1.+0.0e+00j]\n",
      " [ 1.+0.0e+00j -1.+0.0e+00j -1.-1.2e-16j ...  1.+2.5e-02j  1.+2.5e-02j\n",
      "  -1.-2.5e-02j]\n",
      " [ 1.+0.0e+00j  1.+0.0e+00j -1.+0.0e+00j ...  1.+4.9e-02j -1.-4.9e-02j\n",
      "  -1.-4.9e-02j]\n",
      " ...\n",
      " [ 1.+0.0e+00j -1.+0.0e+00j -1.-1.2e-16j ... -1.+7.4e-02j -1.+7.4e-02j\n",
      "   1.-7.4e-02j]\n",
      " [ 1.+0.0e+00j  1.+0.0e+00j -1.+0.0e+00j ... -1.+4.9e-02j  1.-4.9e-02j\n",
      "   1.-4.9e-02j]\n",
      " [ 1.+0.0e+00j -1.+0.0e+00j  1.+1.2e-16j ... -1.+2.5e-02j  1.-2.5e-02j\n",
      "  -1.+2.5e-02j]]\n",
      "First factorization pass\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 530.657, Loss: 530.657, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 508.135, Loss: 508.135, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 481.914, Loss: 481.914, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 349.514, Loss: 349.514, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 48.7386, Loss: 48.7386, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.586183, Loss: 0.586183, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.328739, Loss: 0.328739, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.21825, Loss: 0.21825, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.394948, Loss: 0.394948, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.271874, Loss: 0.271874, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.273093, Loss: 0.273093, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.32562, Loss: 0.32562, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.195322, Loss: 0.195322, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.295116, Loss: 0.295116, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n",
      "SparseFactorizationWithL1AndPruningPytorch: Frob error: 0.517054, Loss: 0.517054, Variables NNzs: [2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 2048], Sum NNzs: 18432\n"
     ]
    }
   ],
   "source": [
    "# Fine grained sparsity on F_32\n",
    "\n",
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import SparseFactorization\n",
    "from SparseFactorization.sparse_factorization import *\n",
    "\n",
    "sz = 512\n",
    "\n",
    "F_matrix, F_true_factors = F_n_no_perm(sz)\n",
    "print(F_matrix)\n",
    "\n",
    "# Sparsity enforcements\n",
    "sparsity_enforcements = {i: np.stack([(v != 0) * 1, (v != 0) * 1], axis=2) for i,v in enumerate(F_true_factors)}\n",
    "\n",
    "# Initializations\n",
    "initializations = {i : np.stack([np.random.normal(0, .01, (sz,sz)), np.random.normal(0, .01, (sz,sz))], axis=2) for i in range(len(sparsity_enforcements.items()))}\n",
    "#initializations = {i : np.stack([np.eye(sz), np.eye(sz)], axis=2) for i in range(len(sparsity_enforcements.items()))}\n",
    "\n",
    "# Factorize!\n",
    "hyperparameters = {\n",
    "    \"learning_rate\" : 5e-3,\n",
    "    \"l1_parameter\" : 0, \n",
    "    \"pruning_threshold\" : 0, \n",
    "    \"training_iters\" : 35000,\n",
    "    \"log_every\" : 100,\n",
    "    \n",
    "    # Here we 4 factors (XYZP)\n",
    "    \"matrix_initializations\" : initializations,\n",
    "    \n",
    "    # Here we apply sparsity constraints according to the sparsity of the butterfly patterns\n",
    "    \"sparsity_constraints\" : sparsity_enforcements,\n",
    "    \n",
    "    # Here we apply value constraints to the permutation matrix. P=permutation matrix\n",
    "    \"value_constraints\" : {\n",
    "    },\n",
    "    \n",
    "    \"is_complex\": True\n",
    "}\n",
    "\n",
    "# First pass\n",
    "print(\"First factorization pass\")\n",
    "factorizer = SparseFactorizationWithEnforcedStructurePytorch(hyperparameters=hyperparameters)\n",
    "recovered_factors, details = factorizer.factorize(to_pytorch(F_matrix))\n",
    "\n",
    "# Visualize each of the recovered_factors and the actual factors\n",
    "recovered_factors = [to_numpy(x) for x in recovered_factors]\n",
    "actual_factors = F_true_factors\n",
    "\n",
    "# Put actual and recovered factors side by side\n",
    "np.set_printoptions(precision=1)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare real parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].real)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].real)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].real)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].real)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].real)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].real)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].real)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].real)\n",
    "\n",
    "print(\"==================\")\n",
    "print(\"Compare imag parts\")\n",
    "print(\"==================\")\n",
    "\n",
    "print(\"Actual 0\\n\", actual_factors[0].imag)\n",
    "print(\"Recovered 0\\n\", recovered_factors[0].imag)\n",
    "\n",
    "print(\"Actual 1\\n\", actual_factors[1].imag)\n",
    "print(\"Recovered 1\\n\", recovered_factors[1].imag)\n",
    "\n",
    "print(\"Actual 2\\n\", actual_factors[2].imag)\n",
    "print(\"Recovered 2\\n\", recovered_factors[2].imag)\n",
    "\n",
    "print(\"Actual 3\\n\", actual_factors[3].imag)\n",
    "print(\"Recovered 3\\n\", recovered_factors[3].imag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
